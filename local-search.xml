<?xml version="1.0" encoding="utf-8"?>
<search>
  
  
  
  <entry>
    <title></title>
    <link href="/2022/08/03/Pytorch-ANN-NN-Start/"/>
    <url>/2022/08/03/Pytorch-ANN-NN-Start/</url>
    
    <content type="html"><![CDATA[<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> torch<br><span class="hljs-keyword">import</span> torch.nn <span class="hljs-keyword">as</span> nn<br><br><span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np<br><span class="hljs-keyword">import</span> matplotlib.pyplot <span class="hljs-keyword">as</span> plt<br>%matplotlib inline<br></code></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs python">x = torch.linspace(<span class="hljs-number">1</span>,<span class="hljs-number">50</span>,<span class="hljs-number">50</span>).reshape(-<span class="hljs-number">1</span>,<span class="hljs-number">1</span>)<br>x<br></code></pre></td></tr></table></figure><pre><code class="hljs">tensor([[ 1.],        [ 2.],        [ 3.],        [ 4.],        [ 5.],        [ 6.],        [ 7.],        [ 8.],        [ 9.],        [10.],        [11.],        [12.],        [13.],        [14.],        [15.],        [16.],        [17.],        [18.],        [19.],        [20.],        [21.],        [22.],        [23.],        [24.],        [25.],        [26.],        [27.],        [28.],        [29.],        [30.],        [31.],        [32.],        [33.],        [34.],        [35.],        [36.],        [37.],        [38.],        [39.],        [40.],        [41.],        [42.],        [43.],        [44.],        [45.],        [46.],        [47.],        [48.],        [49.],        [50.]])</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs python">torch.manual_seed(<span class="hljs-number">71</span>)<br>e = torch.randint(-<span class="hljs-number">8</span>,<span class="hljs-number">9</span>,(<span class="hljs-number">50</span>,<span class="hljs-number">1</span>),dtype=torch.<span class="hljs-built_in">float</span>)<br>e<br></code></pre></td></tr></table></figure><pre><code class="hljs">tensor([[ 2.],        [ 7.],        [ 2.],        [ 6.],        [ 2.],        [-4.],        [ 2.],        [-5.],        [ 4.],        [ 1.],        [ 2.],        [ 3.],        [ 1.],        [-8.],        [ 5.],        [ 5.],        [-6.],        [ 0.],        [-7.],        [-8.],        [-3.],        [-1.],        [ 2.],        [-6.],        [-3.],        [ 3.],        [ 2.],        [ 3.],        [ 4.],        [ 5.],        [ 1.],        [ 7.],        [ 6.],        [-1.],        [-6.],        [-5.],        [-3.],        [ 7.],        [ 0.],        [ 8.],        [-1.],        [-2.],        [ 2.],        [-8.],        [-1.],        [ 6.],        [-8.],        [-3.],        [-7.],        [-2.]])</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs python">y = <span class="hljs-number">2</span>*x + <span class="hljs-number">1</span> + e<br>y<br></code></pre></td></tr></table></figure><pre><code class="hljs">tensor([[ 5.],        [12.],        [ 9.],        [15.],        [13.],        [ 9.],        [17.],        [12.],        [23.],        [22.],        [25.],        [28.],        [28.],        [21.],        [36.],        [38.],        [29.],        [37.],        [32.],        [33.],        [40.],        [44.],        [49.],        [43.],        [48.],        [56.],        [57.],        [60.],        [63.],        [66.],        [64.],        [72.],        [73.],        [68.],        [65.],        [68.],        [72.],        [84.],        [79.],        [89.],        [82.],        [83.],        [89.],        [81.],        [90.],        [99.],        [87.],        [94.],        [92.],        [99.]])</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs python">plt.scatter(x.numpy(),y.numpy())<br></code></pre></td></tr></table></figure><pre><code class="hljs">&lt;matplotlib.collections.PathCollection at 0x1febe8f8438&gt;</code></pre><p><img src="/Untitled_files/Untitled_4_1.png" alt="png"></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs python">torch.manual_seed(<span class="hljs-number">59</span>)<br>model=nn.Linear(in_features=<span class="hljs-number">1</span>,out_features=<span class="hljs-number">1</span>)<br><span class="hljs-built_in">print</span>(model.weight)<br><span class="hljs-built_in">print</span>(model.bias)<br></code></pre></td></tr></table></figure><pre><code class="hljs">Parameter containing:tensor([[0.1060]], requires_grad=True)Parameter containing:tensor([0.9638], requires_grad=True)</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">class</span> <span class="hljs-title class_">Model</span>(nn.Module):<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self,in_features,out_features</span>):<br>        <span class="hljs-built_in">super</span>().__init__()<br>        self.linear = nn.Linear(in_features,out_features)<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">forward</span>(<span class="hljs-params">self,x</span>):<br>        y_pred = self.linear(x)<br>        <span class="hljs-keyword">return</span> y_pred<br>    <br></code></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs python">torch.manual_seed(<span class="hljs-number">59</span>)<br>model=Model(<span class="hljs-number">1</span>,<span class="hljs-number">1</span>)<br><br><span class="hljs-built_in">print</span>(model.linear.weight)<br><span class="hljs-built_in">print</span>(model.linear.bias)<br></code></pre></td></tr></table></figure><pre><code class="hljs">Parameter containing:tensor([[0.1060]], requires_grad=True)Parameter containing:tensor([0.9638], requires_grad=True)</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">for</span> name,param <span class="hljs-keyword">in</span> model.named_parameters():<br>    <span class="hljs-built_in">print</span>(name,<span class="hljs-string">&#x27;\t&#x27;</span>,param.item())<br></code></pre></td></tr></table></figure><pre><code class="hljs">linear.weight  0.10597813129425049linear.bias  0.9637961387634277</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs python">x = torch.tensor([<span class="hljs-number">2.0</span>])<br><span class="hljs-built_in">print</span>(model.forward(x))<br></code></pre></td></tr></table></figure><pre><code class="hljs">tensor([1.1758], grad_fn=&lt;AddBackward0&gt;)</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs python">x1 = np.linspace(<span class="hljs-number">0.0</span>,<span class="hljs-number">50.0</span>,<span class="hljs-number">50</span>)<br>x1<br></code></pre></td></tr></table></figure><pre><code class="hljs">array([ 0.        ,  1.02040816,  2.04081633,  3.06122449,  4.08163265,        5.10204082,  6.12244898,  7.14285714,  8.16326531,  9.18367347,       10.20408163, 11.2244898 , 12.24489796, 13.26530612, 14.28571429,       15.30612245, 16.32653061, 17.34693878, 18.36734694, 19.3877551 ,       20.40816327, 21.42857143, 22.44897959, 23.46938776, 24.48979592,       25.51020408, 26.53061224, 27.55102041, 28.57142857, 29.59183673,       30.6122449 , 31.63265306, 32.65306122, 33.67346939, 34.69387755,       35.71428571, 36.73469388, 37.75510204, 38.7755102 , 39.79591837,       40.81632653, 41.83673469, 42.85714286, 43.87755102, 44.89795918,       45.91836735, 46.93877551, 47.95918367, 48.97959184, 50.        ])</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs python">w1 = <span class="hljs-number">0.1059</span><br>b1 = <span class="hljs-number">0.9637</span><br><br>y1 = w1*x1 + b1<br>y1<br></code></pre></td></tr></table></figure><pre><code class="hljs">array([0.9637    , 1.07176122, 1.17982245, 1.28788367, 1.3959449 ,       1.50400612, 1.61206735, 1.72012857, 1.8281898 , 1.93625102,       2.04431224, 2.15237347, 2.26043469, 2.36849592, 2.47655714,       2.58461837, 2.69267959, 2.80074082, 2.90880204, 3.01686327,       3.12492449, 3.23298571, 3.34104694, 3.44910816, 3.55716939,       3.66523061, 3.77329184, 3.88135306, 3.98941429, 4.09747551,       4.20553673, 4.31359796, 4.42165918, 4.52972041, 4.63778163,       4.74584286, 4.85390408, 4.96196531, 5.07002653, 5.17808776,       5.28614898, 5.3942102 , 5.50227143, 5.61033265, 5.71839388,       5.8264551 , 5.93451633, 6.04257755, 6.15063878, 6.2587    ])</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs python">plt.scatter(x.numpy(),y.numpy())<br>plt.plot(x1,y1,<span class="hljs-string">&#x27;r&#x27;</span>)<br></code></pre></td></tr></table></figure><pre><code class="hljs">[&lt;matplotlib.lines.Line2D at 0x1fec130c0f0&gt;]</code></pre><p><img src="/Untitled_files/Untitled_12_1.png" alt="png"></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs python">criterion = nn.MSELoss()<br></code></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs python">optimizer = torch.optim.SGD(model.parameters(),lr=<span class="hljs-number">0.001</span>)<br></code></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><code class="hljs python">epochs = <span class="hljs-number">50</span><br>losses = []<br><span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(epochs):<br>    i  = i+ <span class="hljs-number">1</span><br>    <br>    y_pred = model.forward(x)<br>    loss = criterion(y_pred,y)<br>    losses.append(loss)<br>    <br>    <span class="hljs-built_in">print</span>(<span class="hljs-string">f&#x27;epoch: <span class="hljs-subst">&#123;i:<span class="hljs-number">2</span>&#125;</span>  loss: <span class="hljs-subst">&#123;loss.item():<span class="hljs-number">10.8</span>f&#125;</span>  weight: <span class="hljs-subst">&#123;model.linear.weight.item():<span class="hljs-number">10.8</span>f&#125;</span>  \</span><br><span class="hljs-string">bias: <span class="hljs-subst">&#123;model.linear.bias.item():<span class="hljs-number">10.8</span>f&#125;</span>&#x27;</span>) <br>    <br>    optimizer.zero_grad()<br>    loss.backward()<br>    optimizer.step()<br></code></pre></td></tr></table></figure><pre><code class="hljs">epoch:  1  loss: 20.98680878  weight: 1.98075056  bias: 1.15903306epoch:  2  loss: 20.98636818  weight: 1.98073113  bias: 1.15969670epoch:  3  loss: 20.98592567  weight: 1.98071122  bias: 1.16035998epoch:  4  loss: 20.98548889  weight: 1.98069167  bias: 1.16102302epoch:  5  loss: 20.98505020  weight: 1.98067176  bias: 1.16168571epoch:  6  loss: 20.98460960  weight: 1.98065245  bias: 1.16234803epoch:  7  loss: 20.98416901  weight: 1.98063242  bias: 1.16301012epoch:  8  loss: 20.98373604  weight: 1.98061311  bias: 1.16367185epoch:  9  loss: 20.98329353  weight: 1.98059309  bias: 1.16433322epoch: 10  loss: 20.98285675  weight: 1.98057389  bias: 1.16499436epoch: 11  loss: 20.98242569  weight: 1.98055387  bias: 1.16565514epoch: 12  loss: 20.98198700  weight: 1.98053443  bias: 1.16631556epoch: 13  loss: 20.98155022  weight: 1.98051476  bias: 1.16697562epoch: 14  loss: 20.98111343  weight: 1.98049510  bias: 1.16763544epoch: 15  loss: 20.98068237  weight: 1.98047566  bias: 1.16829491epoch: 16  loss: 20.98024559  weight: 1.98045588  bias: 1.16895401epoch: 17  loss: 20.97981262  weight: 1.98043656  bias: 1.16961288epoch: 18  loss: 20.97937393  weight: 1.98041677  bias: 1.17027140epoch: 19  loss: 20.97893906  weight: 1.98039722  bias: 1.17092955epoch: 20  loss: 20.97850800  weight: 1.98037779  bias: 1.17158747epoch: 21  loss: 20.97807693  weight: 1.98035824  bias: 1.17224503epoch: 22  loss: 20.97764206  weight: 1.98033857  bias: 1.17290223epoch: 23  loss: 20.97721100  weight: 1.98031914  bias: 1.17355919epoch: 24  loss: 20.97678375  weight: 1.98029959  bias: 1.17421579epoch: 25  loss: 20.97634888  weight: 1.98028016  bias: 1.17487204epoch: 26  loss: 20.97591782  weight: 1.98026061  bias: 1.17552805epoch: 27  loss: 20.97548676  weight: 1.98024118  bias: 1.17618370epoch: 28  loss: 20.97505951  weight: 1.98022163  bias: 1.17683899epoch: 29  loss: 20.97463226  weight: 1.98020220  bias: 1.17749405epoch: 30  loss: 20.97420120  weight: 1.98018289  bias: 1.17814875epoch: 31  loss: 20.97377014  weight: 1.98016334  bias: 1.17880309epoch: 32  loss: 20.97334862  weight: 1.98014390  bias: 1.17945719epoch: 33  loss: 20.97291756  weight: 1.98012447  bias: 1.18011093epoch: 34  loss: 20.97248459  weight: 1.98010504  bias: 1.18076432epoch: 35  loss: 20.97206306  weight: 1.98008573  bias: 1.18141747epoch: 36  loss: 20.97163200  weight: 1.98006618  bias: 1.18207026epoch: 37  loss: 20.97120857  weight: 1.98004699  bias: 1.18272269epoch: 38  loss: 20.97078705  weight: 1.98002744  bias: 1.18337488epoch: 39  loss: 20.97036171  weight: 1.98000824  bias: 1.18402672epoch: 40  loss: 20.96993065  weight: 1.97998869  bias: 1.18467820epoch: 41  loss: 20.96951103  weight: 1.97996950  bias: 1.18532944epoch: 42  loss: 20.96908951  weight: 1.97995007  bias: 1.18598032epoch: 43  loss: 20.96866226  weight: 1.97993064  bias: 1.18663085epoch: 44  loss: 20.96823883  weight: 1.97991157  bias: 1.18728113epoch: 45  loss: 20.96782303  weight: 1.97989213  bias: 1.18793106epoch: 46  loss: 20.96739578  weight: 1.97987282  bias: 1.18858075epoch: 47  loss: 20.96697426  weight: 1.97985351  bias: 1.18923008epoch: 48  loss: 20.96655464  weight: 1.97983420  bias: 1.18987906epoch: 49  loss: 20.96613312  weight: 1.97981501  bias: 1.19052780epoch: 50  loss: 20.96570778  weight: 1.97979569  bias: 1.19117618</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs python">plt.plot(<span class="hljs-built_in">range</span>(epochs), losses)<br>plt.ylabel(<span class="hljs-string">&#x27;Loss&#x27;</span>)<br>plt.xlabel(<span class="hljs-string">&#x27;epoch&#x27;</span>);<br></code></pre></td></tr></table></figure><p><img src="/Untitled_files/Untitled_16_0.png" alt="png"></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><code class="hljs python">w1,b1 = model.linear.weight.item(), model.linear.bias.item()<br><span class="hljs-built_in">print</span>(<span class="hljs-string">f&#x27;Current weight: <span class="hljs-subst">&#123;w1:<span class="hljs-number">.8</span>f&#125;</span>, Current bias: <span class="hljs-subst">&#123;b1:<span class="hljs-number">.8</span>f&#125;</span>&#x27;</span>)<br><span class="hljs-built_in">print</span>()<br><br>y1 = x1*w1 + b1<br><span class="hljs-built_in">print</span>(x1)<br><span class="hljs-built_in">print</span>(y1)<br></code></pre></td></tr></table></figure><pre><code class="hljs">Current weight: 1.97977650, Current bias: 1.19182420[ 0.          1.02040816  2.04081633  3.06122449  4.08163265  5.10204082  6.12244898  7.14285714  8.16326531  9.18367347 10.20408163 11.2244898 12.24489796 13.26530612 14.28571429 15.30612245 16.32653061 17.34693878 18.36734694 19.3877551  20.40816327 21.42857143 22.44897959 23.46938776 24.48979592 25.51020408 26.53061224 27.55102041 28.57142857 29.59183673 30.6122449  31.63265306 32.65306122 33.67346939 34.69387755 35.71428571 36.73469388 37.75510204 38.7755102  39.79591837 40.81632653 41.83673469 42.85714286 43.87755102 44.89795918 45.91836735 46.93877551 47.95918367 48.97959184 50.        ][  1.1918242    3.2120043    5.23218441   7.25236451   9.27254461  11.29272472  13.31290482  15.33308492  17.35326503  19.37344513  21.39362524  23.41380534  25.43398544  27.45416555  29.47434565  31.49452575  33.51470586  35.53488596  37.55506606  39.57524617  41.59542627  43.61560638  45.63578648  47.65596658  49.67614669  51.69632679  53.71650689  55.736687    57.7568671   59.77704721  61.79722731  63.81740741  65.83758752  67.85776762  69.87794772  71.89812783  73.91830793  75.93848804  77.95866814  79.97884824  81.99902835  84.01920845  86.03938855  88.05956866  90.07974876  92.09992887  94.12010897  96.14028907  98.16046918 100.18064928]</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs python">plt.scatter(x.numpy(), y.numpy())<br>plt.plot(x1,y1,<span class="hljs-string">&#x27;r&#x27;</span>)<br>plt.title(<span class="hljs-string">&#x27;Current Model&#x27;</span>)<br>plt.ylabel(<span class="hljs-string">&#x27;y&#x27;</span>)<br>plt.xlabel(<span class="hljs-string">&#x27;x&#x27;</span>);<br></code></pre></td></tr></table></figure><p><img src="/Untitled_files/Untitled_18_0.png" alt="png"></p>]]></content>
    
    
    
  </entry>
  
  
  
  <entry>
    <title>Tensor Basics</title>
    <link href="/2022/08/02/Pytorch-tensor/"/>
    <url>/2022/08/02/Pytorch-tensor/</url>
    
    <content type="html"><![CDATA[<h1 id="Tensor-Basics"><a href="#Tensor-Basics" class="headerlink" title="Tensor Basics"></a>Tensor Basics</h1><h2 id="什么是Tensor"><a href="#什么是Tensor" class="headerlink" title="什么是Tensor"></a>什么是Tensor</h2><p>Tensor实际上就是Numpy中的数组在Pytorch中的一种形式，根据维度的不同有一些特殊的叫法</p><p>下边是一些常见的操作，用以记录<br>查看tensor版本：</p><p><code>torch.__version__</code></p><p>从Numpy Array 生成Tensor<br><strong>注意这里的变量传递是指针，实际上指向一个内存地址，因此任何对arr的修改都会作用在x上</strong></p><p><code>x = torch.from_numpy(arr)</code></p><h2 id="torch-Tensor-vs-torch-tensor"><a href="#torch-Tensor-vs-torch-tensor" class="headerlink" title="torch.Tensor vs. torch.tensor"></a>torch.Tensor vs. torch.tensor</h2><p><code>torch.tensor()</code>生成的tensor默认为int32位<br><code>torch.Tensor()</code>生成的tensor默认为float32位</p><h2 id="tensor的形状修改"><a href="#tensor的形状修改" class="headerlink" title="tensor的形状修改"></a>tensor的形状修改</h2><p>对于多维的tensor，可以通过<code>view</code>或<code>reshape</code>进行修改，也都是浅拷贝<br><code>x = x.view(2,3)</code>  </p><p>当<code>-1</code>参数被传入，代表这个维度被省略，根据另一个传入的参数自动计算<br><code>x = x.view(2,-1)</code></p><h2 id="tensor的运算"><a href="#tensor的运算" class="headerlink" title="tensor的运算"></a>tensor的运算</h2><p>相乘<code>a.mul(b)</code>  </p><p>相乘(但会改变a)<code>a.mul_(b)</code></p><p>点乘<code>a.mm(b)</code>&#x3D;<code>torch.mm(a,b)</code>&#x3D;<code>a @ b</code></p><p>查看tensor长度<code>x.numel()</code></p><p>normalize<code>x.norm()</code></p>]]></content>
    
    
    
    <tags>
      
      <tag>Pytorch</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Artifical Nerual Network</title>
    <link href="/2022/08/02/Pytorch-ANN-Basic/"/>
    <url>/2022/08/02/Pytorch-ANN-Basic/</url>
    
    <content type="html"><![CDATA[<h2 id="感知机模型"><a href="#感知机模型" class="headerlink" title="感知机模型"></a>感知机模型</h2><hr><p><img src="/img/perceptron.png"><br>接受多个输入，给这些输入添加权重和偏置(bias)并输出一个输出的函数就是一个感知机模型，也可以称为一个神经元  </p><h2 id="多层感知机模型"><a href="#多层感知机模型" class="headerlink" title="多层感知机模型"></a>多层感知机模型</h2><hr><p><img src="/img/MultiLayerPerceptron.png"><br>多个单层感知机的组合就是多层感知机，其中中间部分被称作隐藏层，隐藏层大于2层的多层感知机也被称为深度神经网络，多层感知机的输出可以是多个  </p><h2 id="激活函数"><a href="#激活函数" class="headerlink" title="激活函数"></a>激活函数</h2><hr><p>多层感知机模型可以拟合任意函数，激活函数的作用，主要是限制多层感知机的output的范围<br>常用的激活函数：</p><ul><li>ReLU</li><li>tanh</li><li>Sigmoid</li></ul><h2 id="分类问题"><a href="#分类问题" class="headerlink" title="分类问题"></a>分类问题</h2><hr><p>在两分类问题中，输出数据可以为0或者1，来表示两个分类，在多分类问题下这个情况就变得比较复杂，多分类问题可以分为以下两类</p><ul><li>Non-Exclusive-Classes （1条数据有多个label）</li><li>Multi-Exclusive-Classes （1条数据只有一个label）</li></ul><p>对于Non-Exclusive-Classes的情况，可以使用Sigmoid激活函数，保证每个输出都在0，1之间，也可以视作该Label的可能性<br>对于Multi-Exclusive-Classes的情况，使用Softmax函数，对output进行处理，在每个输出都在0，1之间的同时，所有的output相加&#x3D;1  </p><hr><p>当我们将输入数据输入到多层感知器中，得到一个输出output后，对于这个output我们有两个问题：</p><ul><li>我们如何评估我们的output</li><li>如何根据这个output更改我们的w和b</li></ul><h2 id="损失函数"><a href="#损失函数" class="headerlink" title="损失函数"></a>损失函数</h2><hr><p>在构建了多层感知机后，对给定的输入数据得到一个输出output，另外，在原本的数据中，存在一个真实的label值，output和label之间存在一定的误差，计算这个误差的函数就是损失函数</p><h2 id="梯度下降"><a href="#梯度下降" class="headerlink" title="梯度下降"></a>梯度下降</h2><p>使用梯度下降的方式，也就是求导的方式计算梯度，求解损失函数的最小值，就是计算损失函数的导数，令该导数&#x3D;0，计算当前的梯度</p><h2 id="Pytorch-自动计算梯度"><a href="#Pytorch-自动计算梯度" class="headerlink" title="Pytorch 自动计算梯度"></a>Pytorch 自动计算梯度</h2><p><code>import torch</code></p><p><code>x = torch.tensor(2.0,requires_grad=True)</code>  </p><p><code>y = 2*x**4 + x**3 + 3*x**2 + 5*x + 1</code></p><p><code>print(y)</code></p><p><code>y.backward()</code></p><p><code>print(x.grad)</code></p>]]></content>
    
    
    
    <tags>
      
      <tag>Pytorch</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>PySpark的基础使用</title>
    <link href="/2022/08/01/%E5%88%9D%E8%AF%86pyspark/"/>
    <url>/2022/08/01/%E5%88%9D%E8%AF%86pyspark/</url>
    
    <content type="html"><![CDATA[<h2 id="什么是PySpark"><a href="#什么是PySpark" class="headerlink" title="什么是PySpark"></a>什么是PySpark</h2><p>故名思意，PySpark是Spark的Python API，方便我们通过python代码操作Spark，那么什么是Spark</p><h3 id="什么是Spark"><a href="#什么是Spark" class="headerlink" title="什么是Spark"></a>什么是Spark</h3><p><strong>Apache Spark</strong> 是一种用于大数据工作负载的分布式开源处理系统。它使用内存中缓存和优化的查询执行方式，可针对任何规模的数据进行快速分析查询。它提供使用 Java、Scala、Python 和 R 语言的开发 API，支持跨多个工作负载重用代码—批处理、交互式查询、实时分析、机器学习和图形处理等。您会发现各行业的众多组织都使用它，其中包括 FINRA、Yelp、Zillow、DataXu、Urban Institute 和 CrowdStrike。Apache Spark 已经成为最受欢迎的大数据分布式处理框架之一，在 2017 年拥有 365000 名会定期参加聚会的会员。</p><p>refer link:  <a href="https://aws.amazon.com/cn/big-data/what-is-spark/">https://aws.amazon.com/cn/big-data/what-is-spark/</a></p><h3 id="Hadoop"><a href="#Hadoop" class="headerlink" title="Hadoop"></a>Hadoop</h3><p>说到Spark其实就离不开Hadoop的话题，Hadoop 是一种开源框架，它将 Hadoop 分布式文件系统 (HDFS) 用于存储，将 YARN 作为管理由不同应用程序所使用的计算资源的方式，并且实现 MapReduce 编程模型来充当执行引擎。 </p><p>Hadoop：</p><ul><li>HDFS[Hadoop Distribute File System]<ul><li>分布式文件系统</li></ul></li><li>MapReduce<ul><li>Hadoop MapReduce 是一种用于处理大数据集的编程模型，它采用并行的分布式算法。MapReduce 所面对的一项挑战是它要通过连续多步骤流程来运行某项作业。在每个步骤中，MapReduce 要读取来自集群的数据，执行操作，并将结果写到 HDFS。因为每个步骤都需要磁盘读取和写入，磁盘 I&#x2F;O 的延迟会导致 MapReduce 作业变慢。</li></ul></li></ul><p>开发 Spark 的初衷就是为了突破 MapReduce 的这些限制，它可以执行内存中处理，减少作业中的步骤数量，并且跨多项并行操作对数据进行重用。借助于 Spark，将数据读取到内存、执行操作和写回结果仅需要一个步骤，大大地加快了执行的速度。Spark 还能使用内存中缓存显著加快在相同数据集上重复调用某函数的机器学习算法的速度，进而重新使用数据。数据重用通过在弹性分布式数据集 (RDD) 上创建数据抽象—DataFrames 得以实现，而弹性分布式数据集是一个缓存在内存中并在多项 Spark 操作中重新使用的对象集合。它大幅缩短了延迟，使 Spark 比 MapReduce 快数倍，在进行机器学习和交互式分析时尤其明显。</p><h2 id="RDD-Resilient-Distribute-Dataset"><a href="#RDD-Resilient-Distribute-Dataset" class="headerlink" title="RDD[Resilient Distribute Dataset]"></a>RDD[Resilient Distribute Dataset]</h2><p>在 Spark 2.0 之前，Spark 的主要编程接口是弹性分布式数据集（RDD）。在 Spark 2.0 之后，RDD 被 Dataset 取代，它像 RDD 一样是强类型的，但在底层进行了更丰富的优化。仍然支持 RDD 接口，您可以在RDD 编程指南中获得更详细的参考。但是，我们强烈建议您改用 Dataset，它的<strong>性能</strong>比 RDD 更好。</p>]]></content>
    
    
    
    <tags>
      
      <tag>pyspark</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>test-blog</title>
    <link href="/2022/07/28/test-blog/"/>
    <url>/2022/07/28/test-blog/</url>
    
    <content type="html"><![CDATA[<p><img src="/test-blog/C_svg_C_5.svg"></p>]]></content>
    
    
    
  </entry>
  
  
  
  
</search>
