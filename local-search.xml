<?xml version="1.0" encoding="utf-8"?>
<search>
  
  
  
  <entry>
    <title>Tensor Basics</title>
    <link href="/2022/08/02/Pytorch-tensor/"/>
    <url>/2022/08/02/Pytorch-tensor/</url>
    
    <content type="html"><![CDATA[<h1 id="Tensor-Basics"><a href="#Tensor-Basics" class="headerlink" title="Tensor Basics"></a>Tensor Basics</h1><h2 id="什么是Tensor"><a href="#什么是Tensor" class="headerlink" title="什么是Tensor"></a>什么是Tensor</h2><p>Tensor实际上就是Numpy中的数组在Pytorch中的一种形式，根据维度的不同有一些特殊的叫法</p><p>下边是一些常见的操作，用以记录<br>查看tensor版本：</p><p><code>torch.__version__</code></p><p>从Numpy Array 生成Tensor<br><strong>注意这里的变量传递是指针，实际上指向一个内存地址，因此任何对arr的修改都会作用在x上</strong></p><p><code>x = torch.from_numpy(arr)</code></p><h2 id="torch-Tensor-vs-torch-tensor"><a href="#torch-Tensor-vs-torch-tensor" class="headerlink" title="torch.Tensor vs. torch.tensor"></a>torch.Tensor vs. torch.tensor</h2><p><code>torch.tensor()</code>生成的tensor默认为int32位<br><code>torch.Tensor()</code>生成的tensor默认为float32位</p><h2 id="tensor的形状修改"><a href="#tensor的形状修改" class="headerlink" title="tensor的形状修改"></a>tensor的形状修改</h2><p>对于多维的tensor，可以通过<code>view</code>或<code>reshape</code>进行修改，也都是浅拷贝<br><code>x = x.view(2,3)</code>  </p><p>当<code>-1</code>参数被传入，代表这个维度被省略，根据另一个传入的参数自动计算<br><code>x = x.view(2,-1)</code></p><h2 id="tensor的运算"><a href="#tensor的运算" class="headerlink" title="tensor的运算"></a>tensor的运算</h2><p>相乘<code>a.mul(b)</code>  </p><p>相乘(但会改变a)<code>a.mul_(b)</code></p><p>点乘<code>a.mm(b)</code>&#x3D;<code>torch.mm(a,b)</code>&#x3D;<code>a @ b</code></p><p>查看tensor长度<code>x.numel()</code></p><p>normalize<code>x.norm()</code></p>]]></content>
    
    
    
    <tags>
      
      <tag>Pytorch</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Artifical Nerual Network</title>
    <link href="/2022/08/02/Pytorch-ANN-Basic/"/>
    <url>/2022/08/02/Pytorch-ANN-Basic/</url>
    
    <content type="html"><![CDATA[<h2 id="感知机模型"><a href="#感知机模型" class="headerlink" title="感知机模型"></a>感知机模型</h2><hr><p><img src="/img/perceptron.png"><br>接受多个输入，给这些输入添加权重和偏置(bias)并输出一个输出的函数就是一个感知机模型，也可以称为一个神经元  </p><h2 id="多层感知机模型"><a href="#多层感知机模型" class="headerlink" title="多层感知机模型"></a>多层感知机模型</h2><hr><p><img src="/img/MultiLayerPerceptron.png"><br>多个单层感知机的组合就是多层感知机，其中中间部分被称作隐藏层，隐藏层大于2层的多层感知机也被称为深度神经网络，多层感知机的输出可以是多个  </p><h2 id="激活函数"><a href="#激活函数" class="headerlink" title="激活函数"></a>激活函数</h2><hr><p>多层感知机模型可以拟合任意函数，激活函数的作用，主要是限制多层感知机的output的范围<br>常用的激活函数：</p><ul><li>ReLU</li><li>tanh</li><li>Sigmoid</li></ul><h2 id="分类问题"><a href="#分类问题" class="headerlink" title="分类问题"></a>分类问题</h2><hr><p>在两分类问题中，输出数据可以为0或者1，来表示两个分类，在多分类问题下这个情况就变得比较复杂，多分类问题可以分为以下两类</p><ul><li>Non-Exclusive-Classes （1条数据有多个label）</li><li>Multi-Exclusive-Classes （1条数据只有一个label）</li></ul><p>对于Non-Exclusive-Classes的情况，可以使用Sigmoid激活函数，保证每个输出都在0，1之间，也可以视作该Label的可能性<br>对于Multi-Exclusive-Classes的情况，使用Softmax函数，对output进行处理，在每个输出都在0，1之间的同时，所有的output相加&#x3D;1  </p><hr><p>当我们将输入数据输入到多层感知器中，得到一个输出output后，对于这个output我们有两个问题：</p><ul><li>我们如何评估我们的output</li><li>如何根据这个output更改我们的w和b</li></ul><h2 id="损失函数"><a href="#损失函数" class="headerlink" title="损失函数"></a>损失函数</h2><hr><p>在构建了多层感知机后，对给定的输入数据得到一个输出output，另外，在原本的数据中，存在一个真实的label值，output和label之间存在一定的误差，计算这个误差的函数就是损失函数</p><h2 id="梯度下降"><a href="#梯度下降" class="headerlink" title="梯度下降"></a>梯度下降</h2><p>使用梯度下降的方式，也就是求导的方式计算梯度，求解损失函数的最小值，就是计算损失函数的导数，令该导数&#x3D;0，计算当前的梯度</p><h2 id="Pytorch-自动计算梯度"><a href="#Pytorch-自动计算梯度" class="headerlink" title="Pytorch 自动计算梯度"></a>Pytorch 自动计算梯度</h2><p><code>import torch</code></p><p><code>x = torch.tensor(2.0,requires_grad=True)</code>  </p><p><code>y = 2*x**4 + x**3 + 3*x**2 + 5*x + 1</code></p><p><code>print(y)</code></p><p><code>y.backward()</code></p><p><code>print(x.grad)</code></p>]]></content>
    
    
    
    <tags>
      
      <tag>Pytorch</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>PySpark的基础使用</title>
    <link href="/2022/08/01/%E5%88%9D%E8%AF%86pyspark/"/>
    <url>/2022/08/01/%E5%88%9D%E8%AF%86pyspark/</url>
    
    <content type="html"><![CDATA[<h2 id="什么是PySpark"><a href="#什么是PySpark" class="headerlink" title="什么是PySpark"></a>什么是PySpark</h2><p>故名思意，PySpark是Spark的Python API，方便我们通过python代码操作Spark，那么什么是Spark</p><h3 id="什么是Spark"><a href="#什么是Spark" class="headerlink" title="什么是Spark"></a>什么是Spark</h3><p><strong>Apache Spark</strong> 是一种用于大数据工作负载的分布式开源处理系统。它使用内存中缓存和优化的查询执行方式，可针对任何规模的数据进行快速分析查询。它提供使用 Java、Scala、Python 和 R 语言的开发 API，支持跨多个工作负载重用代码—批处理、交互式查询、实时分析、机器学习和图形处理等。您会发现各行业的众多组织都使用它，其中包括 FINRA、Yelp、Zillow、DataXu、Urban Institute 和 CrowdStrike。Apache Spark 已经成为最受欢迎的大数据分布式处理框架之一，在 2017 年拥有 365000 名会定期参加聚会的会员。</p><p>refer link:  <a href="https://aws.amazon.com/cn/big-data/what-is-spark/">https://aws.amazon.com/cn/big-data/what-is-spark/</a></p><h3 id="Hadoop"><a href="#Hadoop" class="headerlink" title="Hadoop"></a>Hadoop</h3><p>说到Spark其实就离不开Hadoop的话题，Hadoop 是一种开源框架，它将 Hadoop 分布式文件系统 (HDFS) 用于存储，将 YARN 作为管理由不同应用程序所使用的计算资源的方式，并且实现 MapReduce 编程模型来充当执行引擎。 </p><p>Hadoop：</p><ul><li>HDFS[Hadoop Distribute File System]<ul><li>分布式文件系统</li></ul></li><li>MapReduce<ul><li>Hadoop MapReduce 是一种用于处理大数据集的编程模型，它采用并行的分布式算法。MapReduce 所面对的一项挑战是它要通过连续多步骤流程来运行某项作业。在每个步骤中，MapReduce 要读取来自集群的数据，执行操作，并将结果写到 HDFS。因为每个步骤都需要磁盘读取和写入，磁盘 I&#x2F;O 的延迟会导致 MapReduce 作业变慢。</li></ul></li></ul><p>开发 Spark 的初衷就是为了突破 MapReduce 的这些限制，它可以执行内存中处理，减少作业中的步骤数量，并且跨多项并行操作对数据进行重用。借助于 Spark，将数据读取到内存、执行操作和写回结果仅需要一个步骤，大大地加快了执行的速度。Spark 还能使用内存中缓存显著加快在相同数据集上重复调用某函数的机器学习算法的速度，进而重新使用数据。数据重用通过在弹性分布式数据集 (RDD) 上创建数据抽象—DataFrames 得以实现，而弹性分布式数据集是一个缓存在内存中并在多项 Spark 操作中重新使用的对象集合。它大幅缩短了延迟，使 Spark 比 MapReduce 快数倍，在进行机器学习和交互式分析时尤其明显。</p><h2 id="RDD-Resilient-Distribute-Dataset"><a href="#RDD-Resilient-Distribute-Dataset" class="headerlink" title="RDD[Resilient Distribute Dataset]"></a>RDD[Resilient Distribute Dataset]</h2><p>在 Spark 2.0 之前，Spark 的主要编程接口是弹性分布式数据集（RDD）。在 Spark 2.0 之后，RDD 被 Dataset 取代，它像 RDD 一样是强类型的，但在底层进行了更丰富的优化。仍然支持 RDD 接口，您可以在RDD 编程指南中获得更详细的参考。但是，我们强烈建议您改用 Dataset，它的<strong>性能</strong>比 RDD 更好。</p>]]></content>
    
    
    
    <tags>
      
      <tag>pyspark</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>test-blog</title>
    <link href="/2022/07/28/test-blog/"/>
    <url>/2022/07/28/test-blog/</url>
    
    <content type="html"><![CDATA[<p><img src="/test-blog/C_svg_C_5.svg"></p>]]></content>
    
    
    
  </entry>
  
  
  
  
</search>
